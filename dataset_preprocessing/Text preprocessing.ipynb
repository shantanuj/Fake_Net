{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "import pandas as pd\n",
    "import csv\n",
    "import keras\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "These functions have already been run to produce relevant corpora, vocabulary, word vectors, etc for our experiments. Thus, there is no need to run them again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making combined stance corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_bodies = pd.read_csv(\"./data/train_bodies.csv\")\n",
    "test_bodies = pd.read_csv(\"./data/test_bodies.csv\")\n",
    "train_headlines = pd.read_csv(\"./data/train.csv\")\n",
    "test_headlines = pd.read_csv(\"./data/test.csv\")\n",
    "combined_bodies = train_bodies.append(test_bodies, ignore_index=True)\n",
    "\n",
    "'''Get combined train and test'''\n",
    "def combined_dataset(pd_headlines, combined_bodies, output_file_name, train=True):\n",
    "    combined = [[\"Headline\",\"Body\",\"Stance\"]]\n",
    "    for index in range(len(pd_headlines)):\n",
    "        if(train):\n",
    "            headline, body_id = pd_headlines.iloc[index].name\n",
    "        else:\n",
    "            headline, body_id = pd_headlines.iloc[index].body, pd_headlines.iloc[index].article \n",
    "        \n",
    "        label = pd_headlines.iloc[index].header  \n",
    "        relevant_body = combined_bodies[combined_bodies['Body ID']==body_id].articleBody.values[0]\n",
    "        combined.append([headline, relevant_body, label])\n",
    "    with open(output_file_name,'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(combined) \n",
    "    \n",
    "combined_dataset(train_headlines, combined_bodies, \"combined_train_stance.csv\", train=True)\n",
    "combined_dataset(test_headlines, combined_bodies, \"combined_test_stance.csv\", train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pruning Word Vectors for task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0x92 in position 81: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-37eadd098e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmake_word_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-37eadd098e97>\u001b[0m in \u001b[0;36mmake_word_vocab\u001b[0;34m(all_texts)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m'''This function creates vocabulary'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#checking if not in vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-37eadd098e97>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(q, lower)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<UNK>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x92 in position 81: invalid start byte"
     ]
    }
   ],
   "source": [
    "label_map = {'agree':0,'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "def tokenize_text(q, lower= False):\n",
    "    '''Function to obtain word tokens'''\n",
    "    try:\n",
    "        tokens = word_tokenize(q.decode('utf-8'))\n",
    "    except:\n",
    "        print(q)\n",
    "        print(\"assigning as UNK due to error\")\n",
    "        return [\"<UNK>\"]\n",
    "    word_tokens = [word for word in tokens if word.isalpha()]  #only include words; not sure if best option\n",
    "    word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "    if(lower):\n",
    "        word_tokens = map(lambda x: x.lower(), word_tokens) #converting all to lower case\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "'''Make the word tokens vocabulary'''\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "def make_word_vocab(all_texts):\n",
    "    '''This function creates vocabulary'''\n",
    "    for text in all_texts:\n",
    "        word_tokens = tokenize_text(text,lower=True)\n",
    "        for token in word_tokens:\n",
    "            if(token not in word_to_int): #checking if not in vocab\n",
    "                word_to_int[token] = len(word_to_int)\n",
    "                int_to_word[len(word_to_int)] = token \n",
    "                \n",
    "                \n",
    "seq_length_list = []\n",
    "\n",
    "'''Stance training set'''\n",
    "#Need to segment into headline, body, label\n",
    "training_df = pd.read_csv(\"Stance_Detection/combined_train_stance.csv\")\n",
    "train_headlines = training_df['Headline'].tolist()\n",
    "train_articles = training_df['Body'].tolist() #store each sequence in list\n",
    "train_labels = map(lambda x: label_map[x],training_df['Stance'].tolist())\n",
    "all_texts = map(lambda x: x[0] + \" \" + x[1], zip(train_headlines, train_articles))  #store concat of headline and article together\n",
    "#convert labels to one hot encoded \n",
    "train_labels = keras.utils.to_categorical(np.asarray(train_labels))\n",
    "\n",
    "\n",
    "'''Stance testing set'''\n",
    "test_df = pd.read_csv(\"Stance_Detection/combined_test_stance.csv\")\n",
    "test_headlines = test_df['Headline'].tolist()\n",
    "test_articles = test_df['Body'].tolist() #store each sequence in list\n",
    "test_labels = map(lambda x: label_map[x],test_df['Stance'].tolist())\n",
    "all_texts += map(lambda x: x[0] + \" \" + x[1], zip(test_headlines, test_articles))  #store concat of headline and article together\n",
    "test_labels = keras.utils.to_categorical(np.asarray(test_labels))\n",
    "\n",
    "\n",
    "'''Input kaggle dataset to get word vectors and vocabulary'''\n",
    "fake_dataset_df = pd.read_csv(\"final_kaggle_combined_dataset.csv\")\n",
    "all_texts += map(lambda x: str(x[0]) + \" \" + str(x[1]), zip(fake_dataset_df['title'].tolist(), fake_dataset_df['text'].tolist()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "make_word_vocab(all_texts)  \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#Learn word vectors through pretrained and continue training to obtain data specific word embeddings --> resolves issue with unknown words. Then, use wmd implementation and simple averaged vector representation.\n",
    "'''\n",
    "word_vectors_loaded = False\n",
    "    \n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"word_vectors/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "ndims = word_vectors.vector_size\n",
    "word_vectors_loaded = True\n",
    "print(\"Loaded word vectors\")\n",
    "\n",
    "def obtain_only_relevant_vectors(word_vectors,vocab):\n",
    "    ndims = word_vectors.vector_size\n",
    "    res_vectors = []\n",
    "    words_done = []\n",
    "    for word in set(vocab):\n",
    "        try:\n",
    "            res_vectors.append((word, list(word_vectors[word])))\n",
    "        except:\n",
    "            continue\n",
    "    with open('corpus_relevant_vectors_.txt', 'w') as f:\n",
    "        f.write(\"{} {}\\n\".format(len(res_vectors), ndims))\n",
    "        print(len(res_vectors))\n",
    "        for word_vec_tuple in res_vectors:\n",
    "            vec = [str(i) for i in word_vec_tuple[1]]\n",
    "            try:\n",
    "                f.write(\"{} {}\\n\".format(str(word_vec_tuple[0]), ' '.join(vec)))\n",
    "            except:\n",
    "                f.write(\"{} {}\\n\".format(word_vec_tuple[0].encode('utf-8'), ' '.join(vec)))\n",
    "        print(\"Finished writing corpus relevant vectors\")\n",
    "obtain_only_relevant_vectors(word_vectors, word_to_int.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Token vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_word_vocab(vocab):\n",
    "    with open('vocab_.json', 'w') as fp:\n",
    "        json.dump(vocab, fp)\n",
    "save_word_vocab(word_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_word_vocab(path=\"vocab.json\"):\n",
    "    with open(path, 'r') as fp:\n",
    "        word_to_int = json.load(fp)\n",
    "        int_to_word = {i:word for word,i in word_to_int.items()}\n",
    "    return word_to_int, int_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
